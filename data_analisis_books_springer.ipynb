{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data analisis books springer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnIklfoO8vdnLeDettK6VU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asuskf/download_books_springer/blob/master/data_analisis_books_springer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jowdZPaHBCgI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Herramientas usadas en el proyecto\n",
        "## Expresiones regulares\n",
        "![Expresiones regulares](https://relopezbriega.github.io/images/regex.png)\n",
        "\n",
        "En cómputo teórico y teoría de lenguajes formales una expresión regular, o expresión racional, ​​ es una secuencia de caracteres que conforma un patrón de búsqueda.\n",
        " \n",
        "**Link de ayuda:** https://relopezbriega.github.io/blog/2015/07/19/expresiones-regulares-con-python/\n",
        "\n",
        "## Beautiful Soup \n",
        "![Beautiful Soup](https://funthon.files.wordpress.com/2017/05/bs.png?w=772)\n",
        "\n",
        "Es una biblioteca de Python para analizar documentos HTML. Esta biblioteca crea un árbol con todos los elementos del documento y puede ser utilizado para extraer información. Por lo tanto, esta biblioteca es útil para realizar web scraping.\n",
        "\n",
        "**Link de ayuda:** https://code.tutsplus.com/es/tutorials/scraping-webpages-in-python-with-beautiful-soup-the-basics--cms-28211\n",
        "\n",
        "## Pandas \n",
        "![Pandas](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Pandas_logo.svg/1200px-Pandas_logo.svg.png)\n",
        "Es una biblioteca de software escrita como extensión de NumPy para manipulación y análisis de datos para el lenguaje de programación Python. \n",
        "\n",
        "**Link de ayuda:** https://www.youtube.com/watch?v=JJ7BMoQotEY&list=PLgHCrivozIb0ULMKfJVV-rFdRG2OeEgfq\n",
        "\n",
        "## NumPy \n",
        "Es una extensión de Python, que le agrega mayor soporte para vectores y matrices, constituyendo una biblioteca de funciones matemáticas de alto nivel para operar con esos vectores o matrices.\n",
        "![NUmpy](https://www.interactivechaos.com/sites/default/files/2019-01/portada_tutorial_numpy.jpg)\n",
        "\n",
        "**Link de ayuda:** https://www.youtube.com/watch?v=WxJr143Os-A\n",
        "\n",
        "## Matplotlib \n",
        "Es una biblioteca para la generación de gráficos a partir de datos contenidos en listas o arrays. \n",
        "![texto alternativo](https://matplotlib.org/_static/logo2_compressed.svg)\n",
        "\n",
        "**Link de ayuda:**  https://matplotlib.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W90lLCbVANcD",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "beautifulsoup4 = !pip list | grep -i beautifulsoup4\n",
        "googledrivedownloader = !pip list | grep -i googledrivedownloader\n",
        "pythonLevenshtein = !pip list | grep -i python-Levenshtein\n",
        "\n",
        "if len(beautifulsoup4) == 0:\n",
        "  !pip install beautifulsoup4\n",
        "\n",
        "if len(googledrivedownloader) == 0:\n",
        "  !pip install googledrivedownloader\n",
        "\n",
        "if len(pythonLevenshtein) == 0:\n",
        "  !pip install python-Levenshtein\n",
        "\n",
        "\n",
        "import requests\n",
        "import shutil\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pathDocument= '/content/libros.csv'\n",
        "\n",
        "if not os.path.exists(pathDocument):\n",
        "  gdd.download_file_from_google_drive(file_id='1slse7jvELBMwoENYhyiUosKBcLofXpgE',\n",
        "                                      dest_path=pathDocument,\n",
        "                                      unzip=False)\n",
        "\n",
        "def downloadBooks (title, link, tipe):\n",
        "  myBook = urlopen(link)\n",
        "  pageResponse = BeautifulSoup(myBook.read(),\"html5lib\")\n",
        "  tags = list(str(pageResponse.findAll('span',{\"class\":\"Keyword\", \"data-test\": \"book-keyword\"})).replace(\" </span\", \"\").split('>'))[1::2]\n",
        "  date = list(str(pageResponse.findAll('span',{\"id\":\"copyright-info\"},{\"class\":\"bibliographic-information__value\"})).replace(\"</span\", \"\").split('>'))[1::2]\n",
        "  year = date[0][-4:]\n",
        "  citacions = list(str(pageResponse.findAll('span',{\"id\":\"bookcitations-count-number\"},{\"class\":\"test-metric-count c-button-circle gtm-bookcitations-count\"})).replace(\"</span\", \"\").split('>'))[1::2]\n",
        "  mentions = list(str(pageResponse.findAll('span',{\"id\":\"bookmentions-count-number\"},{\"class\":\"test-metric-count c-button-circle gtm-bookmentions-count\"})).replace(\"</span\", \"\").split('>'))[1::2]\n",
        "  dowloads = list(str(pageResponse.findAll('span',{\"class\":\"test-metric-count article-metrics__views\"})).replace(\"</span\", \"\").split('>'))[1::2]\n",
        "  #views = dowloads[0][:-1]\n",
        "  views = dowloads[0]\n",
        "  views = re.findall('\\d+', views )\n",
        "  propertiesList = [citacions, mentions, views]\n",
        "  propertiesList = [0 if len(properties)==0 else int(properties[0]) for properties in propertiesList]\n",
        "  importanceBook = (propertiesList[0] * 20)+ (propertiesList[1]*40) + (propertiesList[2] * 60)\n",
        "  autors = list(str(pageResponse.findAll('span',{\"class\":\"authors__name\"}, {\"itemprop\":\"name\"})).replace(\"</span\", \"\").split('>'))[1::2]\n",
        "  nameAutors = [autor.replace('\\xa0',' ') for autor in autors]\n",
        "  \"\"\"\n",
        "  filterResponse = [link['href'] for link in pageResponse.findAll('a', href=True) if re.findall(r'pdf$', link['href'])]\n",
        "  url = 'https://link.springer.com'+filterResponse[0]\n",
        "  path_books = '/content/%s'% (tipe)\n",
        "  \n",
        "  if not os.path.exists(path_books):\n",
        "    os.mkdir(path_books)\n",
        "  \n",
        "  book = requests.get(url, stream=True)\n",
        "  \n",
        "  with open('%s' % (path_books+'/'+title.replace(\" \", \"_\").replace(\"/\", \"_\")+'.pdf'), 'wb') as f:\n",
        "    f.write(requests.get(url).content)\n",
        "  \n",
        "  return tags, year, importanceBook, nameAutors\n",
        "  \"\"\"\n",
        "\n",
        "books = pd.read_csv(\"libros.csv\")\n",
        "type_books = 'Computer Science' #@param ['Behavioral Science and Psychology',  'Religion and Philosophy',  'Intelligent Technologies and Robotics',  'Computer Science',  'Business and Economics',  'Humanities, Social Sciences and Law',  'Energy',  'Mathematics and Statistics',  'Earth and Environmental Science',  'Medicine',  'Economics and Finance',  'Physics and Astronomy',  'Biomedical and Life Sciences',  'Literature, Cultural and Media Studies',  'Engineering',  'Social Sciences',  'Chemistry and Materials Science',  'Business and Management',  'Behavioral Science',  'Education',  'Law and Criminology'] {allow-input: true}\n",
        "leaked_books = books[books['English Package Name'] == type_books]\n",
        "books = dict(zip(leaked_books['Book Title'], leaked_books['OpenURL'].values.tolist()))\n",
        "allTags = [downloadBooks(title, link, type_books) for title, link in books.items()]\n",
        "dowloads= [tags for tags, years, importance, nameAutors in allTags]\n",
        "year= [years for tags, years, importance, nameAutors in allTags]\n",
        "importanceBook = [importance for tags, years, importance, nameAutors in allTags]\n",
        "nameAutors = [nameAutors for tags, years, importance, nameAutors in allTags]\n",
        "\"\"\"\n",
        "shutil.make_archive(type_books, 'zip', '/content/%s'% (type_books))\n",
        "print('Descarga completa')\n",
        "\"\"\"\n",
        "frecuenciaPalab2 = {word.capitalize().replace(\"-\",\" \"):listTags.count(word) for word in listTags}\n",
        "df2 = pd.DataFrame(frecuenciaPalab2.items(), columns=['palabra', 'frecuencia'])\n",
        "df2.sort_values('frecuencia',ascending=False)\n",
        "\n",
        "listTags = [tags for listTags in dowloads for tags in listTags]\n",
        "frecuenciaPalab2 = {word.capitalize().replace(\"-\",\" \"):listTags.count(word) for word in listTags}\n",
        "df2 = pd.DataFrame(frecuenciaPalab2.items(), columns=['palabra', 'frecuencia'])\n",
        "df2.sort_values('frecuencia',ascending=False)\n",
        "\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "listLastWord = []\n",
        "def replaceWords(word, listAllWords):\n",
        "  listLastWord.append(word)\n",
        "  findWord = [word for element in listLastWord if element == word]\n",
        "  listAllWords.remove(word)\n",
        "  wordsByReplace = {extractWord : word  for extractWord in listAllWords if levenshtein_distance(extractWord, word) < 2}\n",
        "  return wordsByReplace\n",
        "listWord = [replaceWords(word, df2['palabra'].values.tolist()) for word in df2['palabra'].values.tolist() if bool(replaceWords(word, df2['palabra'].values.tolist()))]\n",
        "newList = {key:value for word in listWord for key, value in word.items()}\n",
        "dropWord = sorted(list(newList))[1::2]\n",
        "\n",
        "for word in dropWord:\n",
        "  del newList[word]\n",
        "\n",
        "def replaceWords(word):\n",
        "  DataClean = [value for key, value in newList.items() if word == key]\n",
        "  if bool(DataClean) == False:\n",
        "    DataClean = word\n",
        "  else:\n",
        "    DataClean = DataClean[0]\n",
        "  return DataClean\n",
        "\n",
        "listTags = [replaceWords(tags.capitalize()) for listTags in dowloads for tags in listTags] \n",
        "frecuenciaPalab2 = {word:listTags.count(word) for word in listTags}\n",
        "df2 = pd.DataFrame(frecuenciaPalab2.items(), columns=['palabra', 'frecuencia'])\n",
        "df2 = df2.sort_values('frecuencia',ascending=False)\n",
        "values = df2.loc[df2['frecuencia'] >=  3]\n",
        "values\n",
        "\n",
        "frecuenciaPalab2 = {word:listTags.count(word) for word in listTags}\n",
        "df2 = pd.DataFrame(frecuenciaPalab2.items(), columns=['palabra', 'frecuencia'])\n",
        "df2 = df2.sort_values('frecuencia',ascending=False)\n",
        "values = df2.loc[df2['frecuencia'] >=  3]\n",
        "\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "objects = values['palabra'].values.tolist()\n",
        "y_pos = np.arange(len(objects))[::-1]\n",
        "performance = values['frecuencia'].values.tolist()\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.6)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Frecuencia')\n",
        "plt.title('Palabras claves')\n",
        "plt.show()\n",
        "\n",
        "date = {word:year.count(word) for word in year}\n",
        "dfYears = pd.DataFrame(date.items(), columns=['Anio', 'frecuencia'])\n",
        "dfYears = dfYears.sort_values('Anio')\n",
        "dfYears\n",
        "\n",
        "anio = dfYears['Anio'].values.tolist()\n",
        "y_pos = np.arange(len(anio))\n",
        "performance = dfYears['frecuencia'].values.tolist()\n",
        "\n",
        "plt.plot(y_pos, performance, alpha=0.6)\n",
        "plt.xticks(y_pos,anio)\n",
        "plt.xlabel('Año')\n",
        "plt.ylabel('Número')\n",
        "plt.title('Número de libros por año')\n",
        "plt.show()\n",
        "\n",
        "better_book = leaked_books[['Book Title', 'Author', 'OpenURL']]\n",
        "better_book = better_book.drop_duplicates(better_book.columns[~better_book.columns.isin(['OpenURL'])],\n",
        "                        keep='first')\n",
        "better_book = better_book.assign(Importance= importanceBook)\n",
        "better_book = better_book.sort_values('Importance',ascending=False)\n",
        "better_book.head(5)\n",
        "allAutors = [autors for names in nameAutors for autors in names]\n",
        "frecuenciaAutors = {word:allAutors.count(word) for word in allAutors}\n",
        "dfAutors = pd.DataFrame(frecuenciaAutors.items(), columns=['Autores', 'frecuencia'])\n",
        "dfAutors = dfAutors.sort_values('frecuencia',ascending=False)\n",
        "dfAutors = dfAutors.loc[dfAutors['frecuencia'] >=  2]\n",
        "dfAutors\n",
        "\n",
        "objects = dfAutors['Autores'].values.tolist()\n",
        "y_pos = np.arange(len(objects))[::-1]\n",
        "performance = dfAutors['frecuencia'].values.tolist()\n",
        "\n",
        "plt.barh(y_pos, performance, align='center', alpha=0.6)\n",
        "plt.yticks(y_pos, objects)\n",
        "plt.xlabel('Frecuencia')\n",
        "plt.title('Autores')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}